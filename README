*********************************************************************************
*** Tag releases guidelines: vXX.YY                                            **
***                                                                            ** 
***     XX: Increases by one with every change that breaks the precedent tag.  **
***         It means, that using exactly the same configuration files and the  **
***         the same scripts, the executables launched have a runtime error    **
***         or return no-sense results.                                        **
***                                                                            **
***     YY: Increases by one with an indetermined bunch of improvements. The   ** 
***         ouptput will probably change (because new functionalities, bug     ** 
***         fixes, ...), but it doesn't break with the last tag release.       **
***                                                                            **
*********************************************************************************


******************************
***************************
**   Changelog    ****
***************************
******************************

Nov. 9th 2011 --  Tag release v0.1 : First version working
  * Synchronised with original-PROOF code (IFCA-UO/WH  MAIN r1.11)
  * Found a bug in PROOF's code: line 566 'imuon' should be 'i' (r1.12)
Nov. 10th 2011--  Tag release v0.2 : Freeze version with MiniTrees (before change to Latinos)
  * Improved sendcluster utility (to send analysis to cluster and gather the outputs)
  * Added haddPlus utility, hadd improved version to deal with InputParamters and TCounterUI
    objects
  * Fixed some methods in order to make the plot utilities work with the outputs
Nov. 11th 2011--  Tag release v0.3 : First version with Latinos
  * TreeManager include electron methods
Nov. 13th 2011--  Tag release v1.0 : Analysis independence of the final state
  * AnalysisVH (will be) base class for a concrete final state analysis, independently of 
  the final state signature
  * TreeManager extracts directly the data members of a TChain without build the Tree structure
  of a file. TO BE DEPRECATED: 
                              - wrappers methods to get the datamembers of a TChain
			      - construction of the TreeManagerMiniTrees code, and as consequence
			      there will be no longer need to launch the datamanagercreator (it will
			      be renamed to extractdatafiles or something like that)
Nov. 16th 2011--  Tag release v2.0 : Deprecated selector dependence of each file
  * Not needed to created a selector specific for each data file, getting directly the values via
  the name of the branch. It slows the process a factor ~2.3. Nevertheless, it introduces clarity and
  do not break the workflow creating a selector for each file (or group of files), also portability
  between the different types of ntuples. It takes ~20m to process ~1.1M when the v01.00 took ~7m to
  process ~1.1M of events.
Nov. 19th 2011-- Tag release v02.01: Included the mixing signatures
  * Still missing deep checks, but shows reasonable results
Nov. 25th 2011-- Tag release v03.00: Included the AnalysisBase class
  * Each concrete analysis inherits from AnalysisBase which take account the interaction with the data
  * It is needed to virtualize some methods in AnalysisBase in order to let choose the concrete analysis
  what wants to store
Dec. 19th 2011-- Tag release v04.00: WZ analysis stable
  * Deep cleaning of WZ analysis and incorporation of new cuts (dR internal bremsstralung). Results show
  good agreement data-MC (see WZ2011_MCFall11 dir)
  * CutLevels is promoted to a package (out of WH_Analysis), the class keeps the cuts order of every 
  analysis.
  * Deprecated TResultsTable package in favor of printtable.py script
  * Analysis workflow up to now:
     1. Call 'datamanagercreator -r <2011|2011A|2011B> -f <mmm|mme|eem|eee>' to create the 
     datafiles containing the path to the root files (Name_datanames.dn)
     2. Create a directory for each final state
     3. Launch 'sendall SIGNAL', where SIGNAL=WZ or WH
     4. Collect all outputs with 'collectall SIGNAL'
     5. Merge all final states into the one (lepton final state) using 'merge3leptonsfs -d dirch1,dirch2,...'
     6. Do the plots and merge all final states using 'plotall SIGNAL' where SIGNAL can be WHnnn or WZ.
        nnn is the mass hypothesis
   You can see what are the actual process calling in each script
Feb. 08th 2012-- Tag release v04.01: Improvements in the analysis and Fakes incorporation
   * Analysis improvements: 
      - Anti internal breamsstraulhung cut
      - Z and W candidate cut levels (in WZ analysis)
   * Management improvements:
      - New scripts for send, collect all the samples at once
      - New histograms added
   * Fakes incorporation:  it is possible to extract a first estimation of fake background using 
   the new -F option in runanalysis. The estimation is done assuming:
      1. p = 1 (prompt ratio)
      2. TTnT ---> nT ==> weight per event: f/(1-f) where f=f(pt,eta) of the noTight lepton of the
         event
      - The manager of this new functionality is FOManager class
      - Also, added new options to the sendcluster, plothisto, printable, ... scripts to deal with
        this new mode 
Apr. 03rd 2012-- Tag release v04.02: Scale factors incorporation and changes in default sample definitions
   * Analysis improvements:
      - Incorporated the WManager class which controls the scale factors (and ideally whatever 
        efficiency related. So it would superseed the FOManager class, but still not solve some
	problems so FOManager is still working): Inclusion of scale factors
      - WZ Analysis has changed the pre-selection cut from require AT LEAST 3 leptons to 
        EXACTLY 3 leptons
      - BDT based-id electrons included in the analysis, removing the cut-based id electrons
      - Changed WZ sample to be an exclusive (fully leptonic) Madgraph (Fall11)
      - Changed (in all the places where required) the default luminosity to the new value: 4922.0pb-1
   * Management improvements: 
      - Shown in the harvesting process (or standard output of runanalysis) the number of muons
        and electrons whose where selected as no Tight in the Fake sample
      - Changed default ouput format to .pdf
      - New script 'getsystematics' to extract the relative differences of several samples (to be 
        used as systematic extraction)
      - New output table format, more compact, besides the old one. Added a new column which shown
        Nobs-Nbkg
Jun. 14th 2012-- Tag release v04.03: Changed default FR matrices. Cosmethics improvements
   * Analysis improvements:
      - Nominals FR matrices: (muons and electrons)
        + ZJets (before MET cut): 35 (30-muons 'cause do not have it)
	+ ttbar (after MET cut): 50
      - Included PPP (ZZ and WZ) substraction to the Fake estimation
      - Included the check: at least one lepton pass the higher-pt trigger requirement  (when
        AtLeast3Leptons level)
      - Included the VGamma (Zgamma + Wgamma) sample 
      - Changed to the [71,111] mass range of Z (PDG-mass +-20)
      - Using BLUE method to calculate a combined cross-section
   * Management imporvements:
      - WManager class manage all kind of factors to be weighted in an event (scale factors, fake
        rates, ...). Deprecated the FOManager
      - Almost all post-processors scripts moved to python 
      - python scripts which can be used as modules (via import) and contain useful functions 
        or classes are names as: 'somename_mod.py' and they are installed in the system (launching
	the setup script, it is possible to use them)	
      - Added several scripts:
        + Utils/promptsubstract.py   Extract PPP contribution to the Fakes samples. Also creates a
	                             warning file inside the modified 'cluster_Fakes' directory 
				     where the raw numbers before the substraction are there
	+ Utils/plothistopy.py       New python script to plot (instead the old .cc one)
	+ Utils/xscalc.py            Cross-section calculation script
	+ Utils/bluemethod.py        Calculation of combined cross-section with the BLUE method
	+ Utils/LatinoStyle_mod.py   Default cosmethics style (it could be used as python module) 
	+ Utils/functionspool_mod.py Pool of useful functions (it could be used as python module)
	+ Utils/systematics_mod.py   Centralized module where to put the systematics values
	+ Utils/kstest.py            Do a Kolmogorov-Smirnov test (very dependent of the input files)
	+ Utils/setZgammaXS.py       Provisional script to deal with the incorrect cross-section 
	                             quoted in the spreadsheet for the VGamma samples	                             
        + Utils/pdfsystematics.cc    Calculate the systematic due to the PDF (not final version yet)
      - A lot of cosmethics improvements
     

TODO:
  A00. Incorporate to the ATleast3Leptons level, the channel information when are dealing
     with mixing channels. Now until we arrive to the Z candidate level the 2mu1e and the
     2e1mu channel behave the same (because is in that level where it is introduced the 
     flavour requirement). This makes impossible to compare data-MC up to Z candidate level --> DONE for WZ
     DO NOW For VH 
  A001. Add the fakes to the VH. IMPORTANT!!
  A03. FIX BUG in printtable: the rounding is not done appropiately. Check it
  A1.Include an algorithm to extract the WZ and ZZ contribution to the Fake estimation (the PPP contribution)
     ---> DONE, but pending of validation
  B. Include the Z+Jets madgraph (including also the needed modifications in the code
     to avoid doble counting, see Ana's email) 
  5. Check with the *.sh.e* the good behaviour of the job
  12.Generalize the sendcluster script such a way it was able to send any kind of cluster job, not only the 
     runanalysis job (see skimfiles.py script to identify what functions could be used as general funtions
     of an abstract class (clustermanager, for instance) and what ones must be implemented in each concrete
     class. So the germinal idea seems to be: clustermanager is an abstract class to deal with the cluster
     but each user who wants to send her/his code must to implement some concrete class which implement
     some user specific methods, the createbash method for instance.
  14.Include a 'resubmit' argument to sendcluster
  15.In order to control how must be called the CutManager::SelectWhateverLeptons methods, these methods
     should be protected to avoid the direct call by any user, the GetNWhateverLepton methods are the 
     interface methods to use not the SelectWhateverLeptons ... 
  18.Store the kind of lepton in the histograms in order to be coherent when plot them. Also store
     the variable name and unit (in the title and with a detailed format: lepton@variablename@unit 
     for instance)
  19.Substitute the plothisto.cpp by a plothisto.py, include the no-dependence of the samples used--->DONE
  20.Add a way to check if a variable has a cut, then the plot should use the min bin edge of that cut
     (see MET plot after all cuts)
  22.Find a way to deal with the control region analysis: now I have to recompile and change the code if
     I want to send a control region.
  23.Change the _selectedWhatever collection just by a _selected collection in the CutManager class.
     I think it will turn out more legible.
  24.Included a way to deal with the madgraph-powheg (exclusive-inclusive) samples
  25.Include in the Scale Factors and Weights (fake rates) a way to deal with different 
     run periods(A, B) because the input files for each period are different
  26.Find a way to deal with the web conection to extract the PU histogram (PUWeight class). It must
     be used with caution and if there are no answer from the web, then used some per default and
     already downloaded local PU histogram.
  37.Included a print method in AnalisisBase summarizing what parameters was used (DumpParameters)
     what fake rate matrix files, what weights files, scale factors, ...
  38.TO BE FIXED: Minor bug in python scripts. When rounding, there is no propagation in values far 
     away from the last significant number-- example: 0.3545  --> print "%.2f" % 0.3545 --> "0.35" 
     instead of "0.36" which is the correct rounding because 0.3545 --> 0.355 --> 0.36, So it is 
     necessary to build a function which round in a correct way
  

